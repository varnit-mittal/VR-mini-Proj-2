{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 · Environment & Configuration  \n",
    "This section prepares the execution environment and defines all hyper-parameters for LoRA-based 4-bit fine-tuning of a BLIP VQA model.\n",
    "\n",
    "* **Hardware binding** – Forces the process to see **only GPU 0** and disables Weights & Biases logging for a completely self-contained run.  \n",
    "* **Core libraries** – Loads PyTorch, Transformers, PEFT, Pandas, TQDM, and PIL— the minimal stack needed for vision-language fine-tuning.  \n",
    "* **Dataset & model paths** – Points to a mini VQA dataset (`fullInput.csv` + image folder) and the *Salesforce/blip-vqa-base* checkpoint.  \n",
    "* **Quantisation & adaptation** – Enables **4-bit QLoRA** (`USE_4BIT=True`) and sets LoRA hyper-parameters (`LORA_R`, `LORA_ALPHA`, …).  \n",
    "* **Training regime** – Three epochs, moderate batch sizes, and a 5 × 10⁻⁵ learning-rate— conservative choices for stable convergence on a small GPU.  \n",
    "* **Output structure** – `OUTPUT_DIR` is the experiment root; LoRA adapters are stored separately in `LORA_DIR` for clean deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T20:26:52.879023Z",
     "iopub.status.busy": "2025-05-15T20:26:52.878549Z",
     "iopub.status.idle": "2025-05-15T20:27:24.357284Z",
     "shell.execute_reply": "2025-05-15T20:27:24.356621Z",
     "shell.execute_reply.started": "2025-05-15T20:26:52.878998Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False Device count: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #Limit visible GPUs to GPU-0 only avoiding accidental multi-GPU allocation on Kaggle.\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import sys\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from bert_score import score\n",
    "import evaluate\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import ( #Hugging Face Transformers objects for BLIP VQA\n",
    "    BlipProcessor,\n",
    "    BlipForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "# PEFT utilities for parameter-efficient fine-tuning (LoRA, QLoRA, etc.)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "DATASET_CSV = '../VR-mini-Proj-2/fullInput.csv'\n",
    "IMAGE_BASE_DIR = '../images/small'\n",
    "MODEL_NAME = \"Salesforce/blip-vqa-base\"\n",
    "USE_4BIT = True\n",
    "BATCH_SIZE = 8\n",
    "EVAL_BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3\n",
    "LR = 5e-5\n",
    "LORA_R = 16 # Rank of the LoRA update matrices\n",
    "LORA_ALPHA = 32 # Scaling factor (α) for LoRA\n",
    "LORA_DROPOUT = 0.05 # Dropout applied to LoRA layers\n",
    "MAX_LEN = 128\n",
    "OUTPUT_DIR = \"./blip_vqa_lora_q_v8\"\n",
    "LORA_DIR = os.path.join(OUTPUT_DIR, \"lora_adapters\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available(),\"Device count:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 · Custom `VQADataset`\n",
    "\n",
    "This dataset class converts the VQA CSV into a PyTorch-friendly `Dataset` so the Hugging Face `Trainer` can stream data efficiently.\n",
    "\n",
    "**What it does, step by step**\n",
    "\n",
    "1. **Index sanitisation** – Iterates through every row and keeps only those whose `filename` actually exists on disk. This prevents runtime crashes caused by missing images.\n",
    "2. **Image–text pairing** – For each remaining row, the corresponding image is loaded with Pillow (`RGB` mode) and paired with its *question* and *answer* strings from the CSV.\n",
    "3. **Tokenisation** – A single `BlipProcessor` handles both the vision preprocessing and the text tokenisation. The ground-truth answer is passed via `text_target`, enabling teacher forcing during training.\n",
    "4. **Padding and truncation** – All sequences are padded or truncated to `MAX_LEN`. Fixed lengths are crucial for mixed-precision training and for reducing GPU memory fragmentation.\n",
    "5. **Return format** – The method returns a dictionary whose keys match the BLIP model’s input signature (`input_ids`, `pixel_values`, `labels`, etc.). Each tensor is squeezed to remove the batch dimension so PyTorch’s default collate function can stack them later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T20:27:58.657308Z",
     "iopub.status.busy": "2025-05-15T20:27:58.656420Z",
     "iopub.status.idle": "2025-05-15T20:27:58.664647Z",
     "shell.execute_reply": "2025-05-15T20:27:58.663604Z",
     "shell.execute_reply.started": "2025-05-15T20:27:58.657283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Thin wrapper around a Pandas DataFrame for Vision-Language Q&A.\n",
    "    Performs on-disk image existence checks and tokenises each sample via\n",
    "    a shared BLIP processor.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, processor, img_dir, max_len):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.proc = processor\n",
    "        self.img_dir = img_dir\n",
    "        self.max_len = max_len\n",
    "        valid = []\n",
    "        for i, r in tqdm(self.df.iterrows(), total=len(self.df), desc=\"Checking images\"): #Filter out rows whose image files are missing\n",
    "            if os.path.exists(os.path.join(img_dir, str(r['filename']))):\n",
    "                valid.append(i)\n",
    "        if not valid:\n",
    "            raise RuntimeError(\"No valid images found.\")\n",
    "        self.df = self.df.loc[valid].reset_index(drop=True) #Keep only rows with existing images\n",
    "\n",
    "    def __len__(self): \n",
    "        \"\"\"\n",
    "        Total number of valid (image, question, answer) triples\n",
    "        \"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load an image, pair it with its question & answer, and return the\n",
    "        processor-encoded tensors expected by BLIP.\n",
    "        \"\"\"\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(os.path.join(self.img_dir, str(row['filename']))).convert(\"RGB\") #Load image as RGB\n",
    "        enc = self.proc( #Tokenise + visual pre-processing in one call\n",
    "            images=img,\n",
    "            text=str(row['question']),\n",
    "            text_target=str(row['answer']), ## ground-truth answer\n",
    "            padding=\"max_length\", #padding to max len\n",
    "            truncation=True, #trunc. to max len\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\", # return val are torch tensor\n",
    "        )\n",
    "        return {k: v.squeeze(0) for k, v in enc.items()} #Remove batch dimension because Trainer will stack later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 · Data, Quantisation, and LoRA\n",
    "\n",
    "* **Load & split** – Read the CSV, verify a `filename` column, then make an 80 / 20 train-val split with `random_state=42`.\n",
    "* **Processor** – Grab `BlipProcessor` with `use_fast=True` for quicker tokenisation.\n",
    "* **4-bit config** – If `USE_4BIT`, load weights in NF4 4-bit; compute in BF16 when supported (else FP16) and enable double-quant for extra compression.\n",
    "* **Model** – Load `BlipForQuestionAnswering` on `cuda:0`, then call `prepare_model_for_kbit_training`.\n",
    "* **LoRA** – Inject rank-16 LoRA adapters (α = 32, 5 % dropout) into attention projection layers; only these adapters will be trainable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T20:28:19.876601Z",
     "iopub.status.busy": "2025-05-15T20:28:19.876305Z",
     "iopub.status.idle": "2025-05-15T20:28:31.946175Z",
     "shell.execute_reply": "2025-05-15T20:28:31.945552Z",
     "shell.execute_reply.started": "2025-05-15T20:28:19.876581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-bit quantization configured (dtype: torch.float16 )\n",
      "trainable params: 3,538,944 || all params: 388,211,516 || trainable%: 0.9116\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATASET_CSV)\n",
    "if 'filename' not in df.columns:\n",
    "    sys.exit(\"ERROR: 'filename' column missing in CSV\")\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42) #80:20 train test split \n",
    "\n",
    "processor = BlipProcessor.from_pretrained(MODEL_NAME, use_fast=True) # Instantiate BLIP processor (handles both image transforms and tokenisation)\n",
    "model_kwargs = {}\n",
    "if USE_4BIT:\n",
    "    quant_cfg = BitsAndBytesConfig( # Configure 4-bit weight loading via bitsandbytes\n",
    "        load_in_4bit=True, \n",
    "        bnb_4bit_quant_type=\"nf4\",  # Normal-Float 4 quant scheme\n",
    "        bnb_4bit_compute_dtype=(\n",
    "            torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "        ),\n",
    "        bnb_4bit_use_double_quant=True, # Additional compression pass\n",
    "    )\n",
    "    model_kwargs[\"quantization_config\"] = quant_cfg\n",
    "    model_kwargs[\"torch_dtype\"] = (torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16)\n",
    "    print(\"4-bit quantization configured (dtype:\", model_kwargs[\"torch_dtype\"], \")\")\n",
    "\n",
    "model = BlipForQuestionAnswering.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",      # pins everything to cuda:0\n",
    "    **model_kwargs # Injects quantisation config\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False) #Insert low-bit-friendly tweaks (casts, hooks, etc.)\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"query\", \"key\", \"value\"],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg) #Inject trainable LoRA adapters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 · Dataset objects and training hyper-parameters\n",
    "\n",
    "* **Datasets** – Wrap the train and validation splits in `VQADataset`, which handles image checks and tokenisation.\n",
    "* **Warm-up / logging** – Warm-up steps are 10 % of total optimizer steps (but at least 100). Logging happens every ~10 % of a training epoch.\n",
    "* **`TrainingArguments`** highlights  \n",
    "  * 3 epochs, batch sizes 8 (train) / 16 (eval).  \n",
    "  * Gradient checkpointing on (non-reentrant) to save memory.  \n",
    "  * Mixed precision (`fp16=True`) when a CUDA GPU is present.  \n",
    "  * Checkpoints and evaluations occur at the end of every epoch; we skip “best-model” selection to reduce disk I/O.  \n",
    "  * Metrics are logged to **TensorBoard** only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T20:29:33.698564Z",
     "iopub.status.busy": "2025-05-15T20:29:33.698251Z",
     "iopub.status.idle": "2025-05-15T20:29:35.987037Z",
     "shell.execute_reply": "2025-05-15T20:29:35.986436Z",
     "shell.execute_reply.started": "2025-05-15T20:29:33.698543Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking images: 100%|██████████| 39708/39708 [00:14<00:00, 2651.60it/s]\n",
      "Checking images: 100%|██████████| 9928/9928 [00:02<00:00, 3561.05it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = VQADataset(train_df, processor, IMAGE_BASE_DIR, MAX_LEN)\n",
    "val_ds   = VQADataset(val_df,   processor, IMAGE_BASE_DIR, MAX_LEN)\n",
    "\n",
    "warmup = max(100, int(0.1 * len(train_ds) * NUM_EPOCHS / BATCH_SIZE))\n",
    "logs   = max(10,  int(len(train_ds) / BATCH_SIZE / 10))\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    warmup_steps=warmup,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=logs,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    # drop load_best_model_at_end & metric_for_best_model\n",
    "    eval_accumulation_steps=1,\n",
    "    prediction_loss_only=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 · Custom `Trainer` and launch\n",
    "\n",
    "* **CustomTrainer** – Subclasses `Trainer` only to expose a global handle (`trainer = self`) and to make `compute_loss` return the model-defined loss without extra keys.\n",
    "* **Trainer instantiation** – Feeds in the LoRA-augmented model, `TrainingArguments`, and our train/val datasets.\n",
    "* **Training loop** – Starts fine-tuning, then saves the LoRA adapter weights and processor to `LORA_DIR`. Any exception prints a traceback and aborts with a non-zero exit code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:58:56.201253Z",
     "iopub.status.busy": "2025-05-15T22:58:56.200897Z",
     "iopub.status.idle": "2025-05-16T02:33:07.909676Z",
     "shell.execute_reply": "2025-05-16T02:33:07.908989Z",
     "shell.execute_reply.started": "2025-05-15T22:58:56.201232Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True Device count: 1\n",
      "4-bit quantization ready: torch.bfloat16\n",
      "trainable params: 3,538,944 || all params: 388,211,516 || trainable%: 0.9116\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7574ee8553c14abfadf13b7eb1553321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking images:   0%|          | 0/39708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b44912d233947a088f2a5360387c904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking images:   0%|          | 0/9928 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 4-bit LoRA fine-tuning…\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14892' max='14892' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14892/14892 3:34:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.481600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.474200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.465100</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA adapters to ./blip_vqa_lora_q_final/lora_adapters\n"
     ]
    }
   ],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Minimal subclass of Hugging Face Trainer.\n",
    "    Adds a global handle for interactive debugging and\n",
    "    returns the model's intrinsic loss without extra bookkeeping.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        global trainer\n",
    "        trainer = self\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Delegate forward pass to the model and extract `.loss`.\n",
    "        This matches Trainer's expected signature while avoiding\n",
    "        additional metric computation overhead.\n",
    "        \"\"\"\n",
    "        out = model(**inputs)\n",
    "        loss = out.loss if hasattr(out, \"loss\") else out[0] # fallback for tuple\n",
    "        return (loss, out) if return_outputs else loss\n",
    "\n",
    "trainer = CustomTrainer(model=model,args=training_args,train_dataset=train_ds,eval_dataset=val_ds)\n",
    "\n",
    "print(\"Starting 4-bit LoRA fine-tuning…\")\n",
    "try:\n",
    "    trainer.train()                                     # main training loop\n",
    "    trainer.model.save_pretrained(LORA_DIR)            # save LoRA adapters\n",
    "    processor.save_pretrained(LORA_DIR)                # save the processor\n",
    "    print(\"Saved LoRA adapters to\", LORA_DIR)\n",
    "except Exception as e:\n",
    "    print(\"Training failed:\", e)\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Training Evaluation Setup\n",
    "\n",
    "This block initializes the post-training evaluation phase. After fine-tuning the vision-language model with LoRA and 4-bit quantization, we now assess its performance using the validation dataset.\n",
    "\n",
    "Key elements:\n",
    "\n",
    "- **`SAVED_LORA_ADAPTER_DIR_EVAL`**: Path to the saved LoRA adapter directory. This allows the model to load only the low-rank adaptation layers for evaluation.\n",
    "- **`USE_4BIT_FOR_BASE_MODEL_EVAL`**: Ensures the base model is loaded in 4-bit precision, consistent with how it was used during training.\n",
    "- **`eval_df_for_final_eval`**: A copy of the original validation dataframe to preserve data integrity during evaluation.\n",
    "- **`eval_processor`**: The processor (tokenizer + image processor) used for preparing inputs. The code attempts to load it from the LoRA adapter directory to ensure compatibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T02:58:57.704180Z",
     "iopub.status.busy": "2025-05-16T02:58:57.703424Z",
     "iopub.status.idle": "2025-05-16T02:58:57.711101Z",
     "shell.execute_reply": "2025-05-16T02:58:57.710388Z",
     "shell.execute_reply.started": "2025-05-16T02:58:57.704154Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Post-Training Evaluation ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Starting Post-Training Evaluation ---\")\n",
    "SAVED_LORA_ADAPTER_DIR_EVAL = LORA_DIR \n",
    "USE_4BIT_FOR_BASE_MODEL_EVAL = USE_4BIT \n",
    "eval_df_for_final_eval = val_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T02:51:55.616941Z",
     "iopub.status.busy": "2025-05-16T02:51:55.616663Z",
     "iopub.status.idle": "2025-05-16T02:51:55.681410Z",
     "shell.execute_reply": "2025-05-16T02:51:55.680834Z",
     "shell.execute_reply.started": "2025-05-16T02:51:55.616921Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation processor loaded from adapter directory.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    eval_processor = BlipProcessor.from_pretrained(SAVED_LORA_ADAPTER_DIR_EVAL, use_fast=True)\n",
    "    print(\"Evaluation processor loaded from adapter directory.\")\n",
    "except Exception as e_proc:\n",
    "    print(f\"Could not load eval processor from adapter dir: {e_proc}. Using training processor.\")\n",
    "    eval_processor = processor # Fallback to the one used in training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Base Model for Final Evaluation\n",
    "\n",
    "This block loads the base vision-language model (`BlipForQuestionAnswering`) for final evaluation.\n",
    "\n",
    "- If enabled, the model is loaded in 4-bit precision using `BitsAndBytesConfig` for efficient memory usage.\n",
    "- Key quantization settings include NF4 format and `float16` compute type.\n",
    "- The model is loaded with `device_map=\"auto\"` to utilize available hardware efficiently.\n",
    "- This setup ensures consistency with the training configuration and supports adapter-based fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T02:52:36.904151Z",
     "iopub.status.busy": "2025-05-16T02:52:36.903827Z",
     "iopub.status.idle": "2025-05-16T02:52:39.187348Z",
     "shell.execute_reply": "2025-05-16T02:52:39.186564Z",
     "shell.execute_reply.started": "2025-05-16T02:52:36.904129Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model for final evaluation...\n",
      "Base model for final eval will be loaded with 4-bit. Compute dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading base model for final evaluation...\")\n",
    "model_kwargs_eval = {}\n",
    "if USE_4BIT_FOR_BASE_MODEL_EVAL:\n",
    "    # Use the same compute_dtype determined during training setup\n",
    "    compute_dtype_eval = model_kwargs.get(\"torch_dtype\", torch.float16)\n",
    "    #quantization configuration for 4-bit inference\n",
    "    quant_cfg_eval = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,                      # Enable 4-bit quantization\n",
    "        bnb_4bit_quant_type=\"nf4\",              # Use NF4 (normal float 4-bit) quantization scheme\n",
    "        bnb_4bit_compute_dtype=compute_dtype_eval,  # Set compute dtype to match training\n",
    "        bnb_4bit_use_double_quant=True,  \n",
    "    )\n",
    "    model_kwargs_eval[\"quantization_config\"] = quant_cfg_eval #Inject quantization config and compute dtype into model loading kwargs\n",
    "    model_kwargs_eval[\"torch_dtype\"] = compute_dtype_eval\n",
    "    print(f\"Base model for final eval will be loaded with 4-bit. Compute dtype: {compute_dtype_eval}\")\n",
    "\n",
    "base_model_eval = BlipForQuestionAnswering.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    **model_kwargs_eval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T02:53:29.218792Z",
     "iopub.status.busy": "2025-05-16T02:53:29.218259Z",
     "iopub.status.idle": "2025-05-16T02:53:29.223016Z",
     "shell.execute_reply": "2025-05-16T02:53:29.222383Z",
     "shell.execute_reply.started": "2025-05-16T02:53:29.218771Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model for final evaluation successfully loaded in 4-bit.\n"
     ]
    }
   ],
   "source": [
    "# Confirm that the base model is correctly loaded in 4-bit mode before proceeding with evaluation\n",
    "if USE_4BIT_FOR_BASE_MODEL_EVAL and hasattr(base_model_eval, 'is_loaded_in_4bit') and base_model_eval.is_loaded_in_4bit:\n",
    "    print(\"Base model for final evaluation successfully loaded in 4-bit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T02:54:13.246303Z",
     "iopub.status.busy": "2025-05-16T02:54:13.245996Z",
     "iopub.status.idle": "2025-05-16T02:54:14.093675Z",
     "shell.execute_reply": "2025-05-16T02:54:14.092901Z",
     "shell.execute_reply.started": "2025-05-16T02:54:13.246281Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best LoRA weights from ./blip_vqa_lora_q_final/lora_adapters for final evaluation...\n",
      "Best LoRA adapters loaded successfully for final evaluation.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading best LoRA weights from {SAVED_LORA_ADAPTER_DIR_EVAL} for final evaluation...\")\n",
    "eval_model = PeftModel.from_pretrained(base_model_eval, SAVED_LORA_ADAPTER_DIR_EVAL) # Load the trained LoRA adapter into the base model for evaluation\n",
    "eval_model.eval() # Set to evaluation mode\n",
    "print(\"Best LoRA adapters loaded successfully for final evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T02:54:27.433696Z",
     "iopub.status.busy": "2025-05-16T02:54:27.432865Z",
     "iopub.status.idle": "2025-05-16T02:54:27.457419Z",
     "shell.execute_reply": "2025-05-16T02:54:27.456803Z",
     "shell.execute_reply.started": "2025-05-16T02:54:27.433671Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BlipForQuestionAnswering(\n",
       "      (vision_model): BlipVisionModel(\n",
       "        (embeddings): BlipVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        )\n",
       "        (encoder): BlipEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x BlipEncoderLayer(\n",
       "              (self_attn): BlipAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (qkv): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
       "                (projection): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): BlipMLP(\n",
       "                (activation_fn): GELUActivation()\n",
       "                (fc1): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (text_encoder): BlipTextModel(\n",
       "        (embeddings): BlipTextEmbeddings(\n",
       "          (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): BlipTextEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x BlipTextLayer(\n",
       "              (attention): BlipTextAttention(\n",
       "                (self): BlipTextSelfAttention(\n",
       "                  (query): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (value): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): BlipTextSelfOutput(\n",
       "                  (dense): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (crossattention): BlipTextAttention(\n",
       "                (self): BlipTextSelfAttention(\n",
       "                  (query): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (value): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): BlipTextSelfOutput(\n",
       "                  (dense): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BlipTextIntermediate(\n",
       "                (dense): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BlipTextOutput(\n",
       "                (dense): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (text_decoder): BlipTextLMHeadModel(\n",
       "        (bert): BlipTextModel(\n",
       "          (embeddings): BlipTextEmbeddings(\n",
       "            (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "            (position_embeddings): Embedding(512, 768)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (encoder): BlipTextEncoder(\n",
       "            (layer): ModuleList(\n",
       "              (0-11): 12 x BlipTextLayer(\n",
       "                (attention): BlipTextAttention(\n",
       "                  (self): BlipTextSelfAttention(\n",
       "                    (query): lora.Linear4bit(\n",
       "                      (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): lora.Linear4bit(\n",
       "                      (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (value): lora.Linear4bit(\n",
       "                      (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (output): BlipTextSelfOutput(\n",
       "                    (dense): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (crossattention): BlipTextAttention(\n",
       "                  (self): BlipTextSelfAttention(\n",
       "                    (query): lora.Linear4bit(\n",
       "                      (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): lora.Linear4bit(\n",
       "                      (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (value): lora.Linear4bit(\n",
       "                      (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (output): BlipTextSelfOutput(\n",
       "                    (dense): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): BlipTextIntermediate(\n",
       "                  (dense): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output): BlipTextOutput(\n",
       "                  (dense): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (cls): BlipTextOnlyMLMHead(\n",
       "          (predictions): BlipTextLMPredictionHead(\n",
       "            (transform): BlipTextPredictionHeadTransform(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (transform_act_fn): GELUActivation()\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:00:57.401511Z",
     "iopub.status.busy": "2025-05-16T03:00:57.400801Z",
     "iopub.status.idle": "2025-05-16T03:00:57.869495Z",
     "shell.execute_reply": "2025-05-16T03:00:57.868961Z",
     "shell.execute_reply.started": "2025-05-16T03:00:57.401487Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset for final evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7e4e374cca4864a7d4cb81b5da89a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking images:   0%|          | 0/9928 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation dataset and dataloader for final evaluation \n",
    "print(\"Creating dataset for final evaluation...\")\n",
    "final_eval_dataset = VQADataset(eval_df_for_final_eval, eval_processor, IMAGE_BASE_DIR, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:03:14.605116Z",
     "iopub.status.busy": "2025-05-16T03:03:14.604436Z",
     "iopub.status.idle": "2025-05-16T03:03:14.608928Z",
     "shell.execute_reply": "2025-05-16T03:03:14.608152Z",
     "shell.execute_reply.started": "2025-05-16T03:03:14.605066Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:09:18.393530Z",
     "iopub.status.busy": "2025-05-16T03:09:18.392791Z",
     "iopub.status.idle": "2025-05-16T03:09:18.397201Z",
     "shell.execute_reply": "2025-05-16T03:09:18.396392Z",
     "shell.execute_reply.started": "2025-05-16T03:09:18.393505Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "predictions_ft = [] #stores model predictions\n",
    "ground_truths_normalized_ft = [] #store normalized ground-truth answers\n",
    "original_indices_ft = [] #track of original sample indices\n",
    "num_batches_eval = math.ceil(len(eval_df_for_final_eval) / EVAL_BATCH_SIZE) #total number of batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Fine-Tuned Model Evaluation\n",
    "\n",
    "This section of code evaluates a fine-tuned model on an image-question dataset. It processes the data in batches, loads the necessary images, and generates predictions. The predictions are then compared to the ground truth answers, and results are stored for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:10:10.758391Z",
     "iopub.status.busy": "2025-05-16T03:10:10.758071Z",
     "iopub.status.idle": "2025-05-16T03:16:57.011930Z",
     "shell.execute_reply": "2025-05-16T03:16:57.011148Z",
     "shell.execute_reply.started": "2025-05-16T03:10:10.758371Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed910a47efb1420caacb40af3c6d57bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Fine-tuned Model:   0%|          | 0/621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # Disable gradient computation  during evaluation\n",
    "    for i in tqdm(range(0, len(eval_df_for_final_eval), EVAL_BATCH_SIZE), total=num_batches_eval, desc=\"Evaluating Fine-tuned Model\"):\n",
    "        batch_df = eval_df_for_final_eval[i:i+EVAL_BATCH_SIZE] #BATCH\n",
    "        batch_images_pil = []\n",
    "        batch_questions = []\n",
    "        current_batch_ground_truths = []\n",
    "        current_batch_original_indices = []\n",
    "\n",
    "        for idx_in_batch, (original_df_idx, row) in enumerate(batch_df.iterrows()):\n",
    "            question = str(row['question'])\n",
    "            true_answer = str(row['answer']).lower().strip()\n",
    "            image_filename = str(row['filename'])\n",
    "            img_path = os.path.join(IMAGE_BASE_DIR, image_filename) #question, answer, filename loaded for the batch\n",
    "\n",
    "            try:\n",
    "                raw_image = Image.open(img_path).convert('RGB') #read the image in RGB and adding to batched list\n",
    "                batch_images_pil.append(raw_image)\n",
    "                batch_questions.append(question)\n",
    "                current_batch_ground_truths.append(true_answer)\n",
    "                current_batch_original_indices.append(original_df_idx)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning (Eval): Image not found at {img_path}. Skipping.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning (Eval): Error loading image {img_path}: {e}. Skipping.\")\n",
    "\n",
    "        if not batch_images_pil: #Skip the batch if there are no valid images loaded\n",
    "            print(f\"Warning (Eval): No valid images for batch starting at {i}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        #processing batch\n",
    "        inputs = eval_processor(images=batch_images_pil, text=batch_questions, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "        outputs = eval_model.generate(**inputs, max_new_tokens=10)\n",
    "        batch_preds_decoded = eval_processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        for pred_idx, decoded_pred in enumerate(batch_preds_decoded):\n",
    "            predicted_answer = decoded_pred.strip().lower()\n",
    "            predicted_answer = re.sub(r'[^\\w\\s]', '', predicted_answer)\n",
    "\n",
    "            true_answer_normalized = current_batch_ground_truths[pred_idx]\n",
    "            true_answer_normalized = re.sub(r'[^\\w\\s]', '', true_answer_normalized)\n",
    "\n",
    "            predictions_ft.append(predicted_answer)\n",
    "            ground_truths_normalized_ft.append(true_answer_normalized)\n",
    "            original_indices_ft.append(current_batch_original_indices[pred_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:16:57.013601Z",
     "iopub.status.busy": "2025-05-16T03:16:57.013330Z",
     "iopub.status.idle": "2025-05-16T03:16:57.021618Z",
     "shell.execute_reply": "2025-05-16T03:16:57.020861Z",
     "shell.execute_reply.started": "2025-05-16T03:16:57.013585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results_ft_df = pd.DataFrame({\n",
    "    'original_index': original_indices_ft,\n",
    "    'predicted_answer_ft': predictions_ft,\n",
    "    'ground_truth_normalized': ground_truths_normalized_ft\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:16:57.022446Z",
     "iopub.status.busy": "2025-05-16T03:16:57.022269Z",
     "iopub.status.idle": "2025-05-16T03:16:57.054781Z",
     "shell.execute_reply": "2025-05-16T03:16:57.054131Z",
     "shell.execute_reply.started": "2025-05-16T03:16:57.022433Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_index</th>\n",
       "      <th>predicted_answer_ft</th>\n",
       "      <th>ground_truth_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7055</td>\n",
       "      <td>hard</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49141</td>\n",
       "      <td>gold</td>\n",
       "      <td>gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40947</td>\n",
       "      <td>three</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12004</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7905</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_index predicted_answer_ft ground_truth_normalized\n",
       "0            7055                hard                    hard\n",
       "1           49141                gold                    gold\n",
       "2           40947               three                   three\n",
       "3           12004                 yes                     yes\n",
       "4            7905                 two                     two"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ft_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:16:57.056460Z",
     "iopub.status.busy": "2025-05-16T03:16:57.056264Z",
     "iopub.status.idle": "2025-05-16T03:16:57.059842Z",
     "shell.execute_reply": "2025-05-16T03:16:57.059104Z",
     "shell.execute_reply.started": "2025-05-16T03:16:57.056445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eval_df = eval_df_for_final_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:16:57.061266Z",
     "iopub.status.busy": "2025-05-16T03:16:57.060768Z",
     "iopub.status.idle": "2025-05-16T03:16:57.146446Z",
     "shell.execute_reply": "2025-05-16T03:16:57.145886Z",
     "shell.execute_reply.started": "2025-05-16T03:16:57.061243Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned results saved to vqa_results_lora_finetuned.csv\n"
     ]
    }
   ],
   "source": [
    "df_with_ft_results = eval_df.merge(results_ft_df, left_index=True, right_on='original_index', how='right')\n",
    "results_ft_filename = 'vqa_results_lora_finetuned.csv'\n",
    "df_with_ft_results.to_csv(results_ft_filename, index=False)\n",
    "print(f\"Fine-tuned results saved to {results_ft_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:16:57.147348Z",
     "iopub.status.busy": "2025-05-16T03:16:57.147108Z",
     "iopub.status.idle": "2025-05-16T03:16:57.152768Z",
     "shell.execute_reply": "2025-05-16T03:16:57.152133Z",
     "shell.execute_reply.started": "2025-05-16T03:16:57.147328Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'question', 'answer', 'filename', 'original_index',\n",
       "       'predicted_answer_ft', 'ground_truth_normalized'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_ft_results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:18:36.604366Z",
     "iopub.status.busy": "2025-05-16T03:18:36.604114Z",
     "iopub.status.idle": "2025-05-16T03:18:57.894425Z",
     "shell.execute_reply": "2025-05-16T03:18:57.893623Z",
     "shell.execute_reply.started": "2025-05-16T03:18:36.604351Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model Accuracy (Exact Match): 0.5701\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdfbdb473cb640d1bf62662d014ab0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1918409653a5436390139bdc62faa349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c694d0bb56e44dbb11ab2f5d4ce97c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74332c1b995841c994097d8d790aa5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23c35305cf342b88992c908629ea3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a36e6d7c13403aaa20d4130d86c905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc907e2b4f3403693e260bcd4e9815b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ BERTScore → P: 0.8724, R: 0.8459, F1: 0.8572\n",
      "▶ BERTScore1 → P1: 0.9785, R: 0.9740, F1: 0.9759\n",
      "LoRA Fine-tuning and Evaluation Script Finished.\n"
     ]
    }
   ],
   "source": [
    "if not predictions_ft:\n",
    "    print(\"Error: No valid fine-tuned predictions available to calculate metrics.\")\n",
    "else:\n",
    "    correct_predictions_ft = sum(p == gt for p, gt in zip(predictions_ft, ground_truths_normalized_ft))\n",
    "    total_valid_ft = len(predictions_ft)\n",
    "    accuracy_ft = correct_predictions_ft / total_valid_ft if total_valid_ft > 0 else 0\n",
    "    pred= df_with_ft_results['predicted_answer_ft']\n",
    "    gt = df_with_ft_results['ground_truth_normalized']\n",
    "    print(f\"Fine-tuned Model Accuracy (Exact Match): {accuracy_ft:.4f}\")\n",
    "    try:\n",
    "        bertscore = evaluate.load(\"bertscore\")\n",
    "        \n",
    "        # P, R, F1_bs  = score(pred.tolist(), gt.tolist(), lang='en', verbose=False,rescale_with_baseline=True)\n",
    "        # avg_f1_bertscore = sum(bertscore_results['f1']) / len(bertscore_results['f1']) if bertscore_results['f1'] else 0\n",
    "        # print(f\"BERTScore - P: {P.mean().item():.4f}, R: {R.mean().item():.4f}, F1: {F1_bs.mean().item():.4f}\")\n",
    "        \n",
    "        \n",
    "        P1, R1, F1_bs1  = score(pred.tolist(), gt.tolist(), lang='en', verbose=False)\n",
    "        print(f\"BERTScore1 - P1: {P1.mean().item():.4f}, R: {R1.mean().item():.4f}, F1: {F1_bs1.mean().item():.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not compute BERTScore for fine-tuned model: {e}\")\n",
    "\n",
    "print(\"LoRA Fine-tuning and Evaluation Script Finished.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7430483,
     "sourceId": 11828111,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tdr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
