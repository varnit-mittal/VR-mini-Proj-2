{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T20:26:52.879023Z",
     "iopub.status.busy": "2025-05-15T20:26:52.878549Z",
     "iopub.status.idle": "2025-05-15T20:27:24.357284Z",
     "shell.execute_reply": "2025-05-15T20:27:24.356621Z",
     "shell.execute_reply.started": "2025-05-15T20:26:52.878998Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False Device count: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Only GPU 0 visible\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# Disable WandB / multi-node\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "import sys\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# --- CONFIG ---\n",
    "DATASET_CSV      = '../VR-mini-Proj-2/fullInput.csv'\n",
    "IMAGE_BASE_DIR   = '../images/small'\n",
    "MODEL_NAME       = \"Salesforce/blip-vqa-base\"\n",
    "USE_4BIT         = True\n",
    "BATCH_SIZE       = 8\n",
    "EVAL_BATCH_SIZE  = 16\n",
    "NUM_EPOCHS       = 3\n",
    "LR               = 5e-5\n",
    "LORA_R           = 16\n",
    "LORA_ALPHA       = 32\n",
    "LORA_DROPOUT     = 0.05\n",
    "MAX_LEN          = 128\n",
    "OUTPUT_DIR       = \"./blip_vqa_lora_q_v8\"\n",
    "LORA_DIR         = os.path.join(OUTPUT_DIR, \"lora_adapters\")\n",
    "DEVICE         = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available(),\n",
    "      \"Device count:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T20:27:58.657308Z",
     "iopub.status.busy": "2025-05-15T20:27:58.656420Z",
     "iopub.status.idle": "2025-05-15T20:27:58.664647Z",
     "shell.execute_reply": "2025-05-15T20:27:58.663604Z",
     "shell.execute_reply.started": "2025-05-15T20:27:58.657283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- DATASET CLASS ---\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, processor, img_dir, max_len):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.proc = processor\n",
    "        self.img_dir = img_dir\n",
    "        self.max_len = max_len\n",
    "        valid = []\n",
    "        for i, r in tqdm(self.df.iterrows(), total=len(self.df), desc=\"Checking images\"):\n",
    "            if os.path.exists(os.path.join(img_dir, str(r['filename']))):\n",
    "                valid.append(i)\n",
    "        if not valid:\n",
    "            raise RuntimeError(\"No valid images found.\")\n",
    "        self.df = self.df.loc[valid].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(os.path.join(self.img_dir, str(row['filename']))).convert(\"RGB\")\n",
    "        enc = self.proc(\n",
    "            images=img,\n",
    "            text=str(row['question']),\n",
    "            text_target=str(row['answer']),\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {k: v.squeeze(0) for k, v in enc.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T20:28:19.876601Z",
     "iopub.status.busy": "2025-05-15T20:28:19.876305Z",
     "iopub.status.idle": "2025-05-15T20:28:31.946175Z",
     "shell.execute_reply": "2025-05-15T20:28:31.945552Z",
     "shell.execute_reply.started": "2025-05-15T20:28:19.876581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-bit quantization configured (dtype: torch.float16 )\n",
      "trainable params: 3,538,944 || all params: 388,211,516 || trainable%: 0.9116\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD & SPLIT CSV ---\n",
    "df = pd.read_csv(DATASET_CSV)\n",
    "if 'filename' not in df.columns:\n",
    "    sys.exit(\"ERROR: 'filename' column missing in CSV\")\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- PROCESSOR & 4-BIT SETUP ---\n",
    "processor = BlipProcessor.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model_kwargs = {}\n",
    "if USE_4BIT:\n",
    "    quant_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=(\n",
    "            torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "        ),\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    model_kwargs[\"quantization_config\"] = quant_cfg\n",
    "    model_kwargs[\"torch_dtype\"] = (\n",
    "        torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    )\n",
    "    print(\"4-bit quantization configured (dtype:\", model_kwargs[\"torch_dtype\"], \")\")\n",
    "\n",
    "# --- LOAD & PREPARE MODEL ---\n",
    "model = BlipForQuestionAnswering.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",      # pins everything to cuda:0\n",
    "    **model_kwargs\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"query\", \"key\", \"value\"],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T20:29:33.698564Z",
     "iopub.status.busy": "2025-05-15T20:29:33.698251Z",
     "iopub.status.idle": "2025-05-15T20:29:35.987037Z",
     "shell.execute_reply": "2025-05-15T20:29:35.986436Z",
     "shell.execute_reply.started": "2025-05-15T20:29:33.698543Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking images: 100%|██████████| 39708/39708 [00:14<00:00, 2651.60it/s]\n",
      "Checking images: 100%|██████████| 9928/9928 [00:02<00:00, 3561.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Datasets ---\n",
    "train_ds = VQADataset(train_df, processor, IMAGE_BASE_DIR, MAX_LEN)\n",
    "val_ds   = VQADataset(val_df,   processor, IMAGE_BASE_DIR, MAX_LEN)\n",
    "\n",
    "# --- TRAINING ARGS (no best-model) ---\n",
    "warmup = max(100, int(0.1 * len(train_ds) * NUM_EPOCHS / BATCH_SIZE))\n",
    "logs   = max(10,  int(len(train_ds) / BATCH_SIZE / 10))\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    warmup_steps=warmup,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=logs,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    # drop load_best_model_at_end & metric_for_best_model\n",
    "    eval_accumulation_steps=1,\n",
    "    prediction_loss_only=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:58:56.201253Z",
     "iopub.status.busy": "2025-05-15T22:58:56.200897Z",
     "iopub.status.idle": "2025-05-16T02:33:07.909676Z",
     "shell.execute_reply": "2025-05-16T02:33:07.908989Z",
     "shell.execute_reply.started": "2025-05-15T22:58:56.201232Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True Device count: 1\n",
      "4-bit quantization ready: torch.bfloat16\n",
      "trainable params: 3,538,944 || all params: 388,211,516 || trainable%: 0.9116\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7574ee8553c14abfadf13b7eb1553321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking images:   0%|          | 0/39708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b44912d233947a088f2a5360387c904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking images:   0%|          | 0/9928 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 4-bit LoRA fine-tuning…\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14892' max='14892' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14892/14892 3:34:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.481600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.474200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.465100</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA adapters to ./blip_vqa_lora_q_final/lora_adapters\n"
     ]
    }
   ],
   "source": [
    "# --- CUSTOM TRAINER (match signature) ---\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        global trainer\n",
    "        trainer = self\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        out = model(**inputs)\n",
    "        loss = out.loss if hasattr(out, \"loss\") else out[0]\n",
    "        return (loss, out) if return_outputs else loss\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    ")\n",
    "\n",
    "# --- RUN TRAINING ---\n",
    "print(\"Starting 4-bit LoRA fine-tuning…\")\n",
    "try:\n",
    "    trainer.train()\n",
    "    trainer.model.save_pretrained(LORA_DIR)\n",
    "    processor.save_pretrained(LORA_DIR)\n",
    "    print(\"Saved LoRA adapters to\", LORA_DIR)\n",
    "except Exception as e:\n",
    "    print(\"Training failed:\", e)\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T02:58:57.704180Z",
     "iopub.status.busy": "2025-05-16T02:58:57.703424Z",
     "iopub.status.idle": "2025-05-16T02:58:57.711101Z",
     "shell.execute_reply": "2025-05-16T02:58:57.710388Z",
     "shell.execute_reply.started": "2025-05-16T02:58:57.704154Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Post-Training Evaluation ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Starting Post-Training Evaluation ---\")\n",
    "\n",
    "# --- CONFIG FOR EVALUATION (Mostly inherited or defined above) ---\n",
    "SAVED_LORA_ADAPTER_DIR_EVAL = LORA_DIR # Adapters just saved\n",
    "USE_4BIT_FOR_BASE_MODEL_EVAL = USE_4BIT # Match training quantization\n",
    "# For evaluation, we'll use the same val_df_for_training split,\n",
    "# or you can define a new df for a dedicated test set.\n",
    "eval_df_for_final_eval = val_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T02:51:55.616941Z",
     "iopub.status.busy": "2025-05-16T02:51:55.616663Z",
     "iopub.status.idle": "2025-05-16T02:51:55.681410Z",
     "shell.execute_reply": "2025-05-16T02:51:55.680834Z",
     "shell.execute_reply.started": "2025-05-16T02:51:55.616921Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation processor loaded from adapter directory.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    eval_processor = BlipProcessor.from_pretrained(SAVED_LORA_ADAPTER_DIR_EVAL, use_fast=True)\n",
    "    print(\"Evaluation processor loaded from adapter directory.\")\n",
    "except Exception as e_proc:\n",
    "    print(f\"Could not load eval processor from adapter dir: {e_proc}. Using training processor.\")\n",
    "    eval_processor = processor # Fallback to the one used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T02:52:36.904151Z",
     "iopub.status.busy": "2025-05-16T02:52:36.903827Z",
     "iopub.status.idle": "2025-05-16T02:52:39.187348Z",
     "shell.execute_reply": "2025-05-16T02:52:39.186564Z",
     "shell.execute_reply.started": "2025-05-16T02:52:36.904129Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model for final evaluation...\n",
      "Base model for final eval will be loaded with 4-bit. Compute dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD BASE MODEL AND APPLY BEST LoRA ADAPTER FOR EVALUATION ---\n",
    "print(\"Loading base model for final evaluation...\")\n",
    "model_kwargs_eval = {}\n",
    "if USE_4BIT_FOR_BASE_MODEL_EVAL:\n",
    "    # Use the same compute_dtype determined during training setup\n",
    "    compute_dtype_eval = model_kwargs.get(\"torch_dtype\", torch.float16)\n",
    "    quant_cfg_eval = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype_eval,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    model_kwargs_eval[\"quantization_config\"] = quant_cfg_eval\n",
    "    model_kwargs_eval[\"torch_dtype\"] = compute_dtype_eval\n",
    "    print(f\"Base model for final eval will be loaded with 4-bit. Compute dtype: {compute_dtype_eval}\")\n",
    "\n",
    "base_model_eval = BlipForQuestionAnswering.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    **model_kwargs_eval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T02:53:29.218792Z",
     "iopub.status.busy": "2025-05-16T02:53:29.218259Z",
     "iopub.status.idle": "2025-05-16T02:53:29.223016Z",
     "shell.execute_reply": "2025-05-16T02:53:29.222383Z",
     "shell.execute_reply.started": "2025-05-16T02:53:29.218771Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model for final evaluation successfully loaded in 4-bit.\n"
     ]
    }
   ],
   "source": [
    "if USE_4BIT_FOR_BASE_MODEL_EVAL and hasattr(base_model_eval, 'is_loaded_in_4bit') and base_model_eval.is_loaded_in_4bit:\n",
    "    print(\"Base model for final evaluation successfully loaded in 4-bit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T02:54:13.246303Z",
     "iopub.status.busy": "2025-05-16T02:54:13.245996Z",
     "iopub.status.idle": "2025-05-16T02:54:14.093675Z",
     "shell.execute_reply": "2025-05-16T02:54:14.092901Z",
     "shell.execute_reply.started": "2025-05-16T02:54:13.246281Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best LoRA weights from ./blip_vqa_lora_q_final/lora_adapters for final evaluation...\n",
      "Best LoRA adapters loaded successfully for final evaluation.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading best LoRA weights from {SAVED_LORA_ADAPTER_DIR_EVAL} for final evaluation...\")\n",
    "eval_model = PeftModel.from_pretrained(base_model_eval, SAVED_LORA_ADAPTER_DIR_EVAL)\n",
    "eval_model.eval() # Set to evaluation mode\n",
    "# Optional: Merge for potentially faster inference\n",
    "# eval_model = eval_model.merge_and_unload()\n",
    "# print(\"LoRA adapters merged into base model for final evaluation.\")\n",
    "print(\"Best LoRA adapters loaded successfully for final evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T02:54:27.433696Z",
     "iopub.status.busy": "2025-05-16T02:54:27.432865Z",
     "iopub.status.idle": "2025-05-16T02:54:27.457419Z",
     "shell.execute_reply": "2025-05-16T02:54:27.456803Z",
     "shell.execute_reply.started": "2025-05-16T02:54:27.433671Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BlipForQuestionAnswering(\n",
       "      (vision_model): BlipVisionModel(\n",
       "        (embeddings): BlipVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        )\n",
       "        (encoder): BlipEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x BlipEncoderLayer(\n",
       "              (self_attn): BlipAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (qkv): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
       "                (projection): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): BlipMLP(\n",
       "                (activation_fn): GELUActivation()\n",
       "                (fc1): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (text_encoder): BlipTextModel(\n",
       "        (embeddings): BlipTextEmbeddings(\n",
       "          (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): BlipTextEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x BlipTextLayer(\n",
       "              (attention): BlipTextAttention(\n",
       "                (self): BlipTextSelfAttention(\n",
       "                  (query): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (value): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): BlipTextSelfOutput(\n",
       "                  (dense): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (crossattention): BlipTextAttention(\n",
       "                (self): BlipTextSelfAttention(\n",
       "                  (query): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (value): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): BlipTextSelfOutput(\n",
       "                  (dense): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BlipTextIntermediate(\n",
       "                (dense): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BlipTextOutput(\n",
       "                (dense): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (text_decoder): BlipTextLMHeadModel(\n",
       "        (bert): BlipTextModel(\n",
       "          (embeddings): BlipTextEmbeddings(\n",
       "            (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "            (position_embeddings): Embedding(512, 768)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (encoder): BlipTextEncoder(\n",
       "            (layer): ModuleList(\n",
       "              (0-11): 12 x BlipTextLayer(\n",
       "                (attention): BlipTextAttention(\n",
       "                  (self): BlipTextSelfAttention(\n",
       "                    (query): lora.Linear4bit(\n",
       "                      (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): lora.Linear4bit(\n",
       "                      (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (value): lora.Linear4bit(\n",
       "                      (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (output): BlipTextSelfOutput(\n",
       "                    (dense): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (crossattention): BlipTextAttention(\n",
       "                  (self): BlipTextSelfAttention(\n",
       "                    (query): lora.Linear4bit(\n",
       "                      (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): lora.Linear4bit(\n",
       "                      (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (value): lora.Linear4bit(\n",
       "                      (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (output): BlipTextSelfOutput(\n",
       "                    (dense): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): BlipTextIntermediate(\n",
       "                  (dense): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output): BlipTextOutput(\n",
       "                  (dense): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (cls): BlipTextOnlyMLMHead(\n",
       "          (predictions): BlipTextLMPredictionHead(\n",
       "            (transform): BlipTextPredictionHeadTransform(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (transform_act_fn): GELUActivation()\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:00:57.401511Z",
     "iopub.status.busy": "2025-05-16T03:00:57.400801Z",
     "iopub.status.idle": "2025-05-16T03:00:57.869495Z",
     "shell.execute_reply": "2025-05-16T03:00:57.868961Z",
     "shell.execute_reply.started": "2025-05-16T03:00:57.401487Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset for final evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7e4e374cca4864a7d4cb81b5da89a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking images:   0%|          | 0/9928 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- CREATE EVALUATION DATASET & DATALOADER FOR FINAL EVAL ---\n",
    "print(\"Creating dataset for final evaluation...\")\n",
    "final_eval_dataset = VQADataset(eval_df_for_final_eval, eval_processor, IMAGE_BASE_DIR, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:03:14.605116Z",
     "iopub.status.busy": "2025-05-16T03:03:14.604436Z",
     "iopub.status.idle": "2025-05-16T03:03:14.608928Z",
     "shell.execute_reply": "2025-05-16T03:03:14.608152Z",
     "shell.execute_reply.started": "2025-05-16T03:03:14.605066Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:09:18.393530Z",
     "iopub.status.busy": "2025-05-16T03:09:18.392791Z",
     "iopub.status.idle": "2025-05-16T03:09:18.397201Z",
     "shell.execute_reply": "2025-05-16T03:09:18.396392Z",
     "shell.execute_reply.started": "2025-05-16T03:09:18.393505Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "predictions_ft = []\n",
    "ground_truths_normalized_ft = []\n",
    "original_indices_ft = []\n",
    "\n",
    "num_batches_eval = math.ceil(len(eval_df_for_final_eval) / EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:10:10.758391Z",
     "iopub.status.busy": "2025-05-16T03:10:10.758071Z",
     "iopub.status.idle": "2025-05-16T03:16:57.011930Z",
     "shell.execute_reply": "2025-05-16T03:16:57.011148Z",
     "shell.execute_reply.started": "2025-05-16T03:10:10.758371Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed910a47efb1420caacb40af3c6d57bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Fine-tuned Model:   0%|          | 0/621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(eval_df_for_final_eval), EVAL_BATCH_SIZE), total=num_batches_eval, desc=\"Evaluating Fine-tuned Model\"):\n",
    "        batch_df = eval_df_for_final_eval[i:i+EVAL_BATCH_SIZE]\n",
    "        \n",
    "        batch_images_pil = []\n",
    "        batch_questions = []\n",
    "        current_batch_ground_truths = [] # Ground truths for this specific batch\n",
    "        current_batch_original_indices = [] # Original indices for this specific batch\n",
    "\n",
    "        for idx_in_batch, (original_df_idx, row) in enumerate(batch_df.iterrows()):\n",
    "            question = str(row['question'])\n",
    "            true_answer = str(row['answer']).lower().strip()\n",
    "            # Use the 'filename' column\n",
    "            image_filename = str(row['filename'])\n",
    "            img_path = os.path.join(IMAGE_BASE_DIR, image_filename)\n",
    "\n",
    "            try:\n",
    "                raw_image = Image.open(img_path).convert('RGB')\n",
    "                batch_images_pil.append(raw_image)\n",
    "                batch_questions.append(question)\n",
    "                current_batch_ground_truths.append(true_answer)\n",
    "                current_batch_original_indices.append(original_df_idx)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning (Eval): Image not found at {img_path}. Skipping.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning (Eval): Error loading image {img_path}: {e}. Skipping.\")\n",
    "\n",
    "        if not batch_images_pil:\n",
    "            print(f\"Warning (Eval): No valid images for batch starting at {i}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        inputs = eval_processor(images=batch_images_pil, text=batch_questions, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "        \n",
    "        # Generate answers\n",
    "        # For LoRA model, if it's merged, call is same. If separate, PEFT handles it.\n",
    "        outputs = eval_model.generate(**inputs, max_new_tokens=10) # Keep consistent with baseline\n",
    "        \n",
    "        batch_preds_decoded = eval_processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        for pred_idx, decoded_pred in enumerate(batch_preds_decoded):\n",
    "            predicted_answer = decoded_pred.strip().lower()\n",
    "            predicted_answer = re.sub(r'[^\\w\\s]', '', predicted_answer)\n",
    "\n",
    "            true_answer_normalized = current_batch_ground_truths[pred_idx]\n",
    "            true_answer_normalized = re.sub(r'[^\\w\\s]', '', true_answer_normalized)\n",
    "\n",
    "            predictions_ft.append(predicted_answer)\n",
    "            ground_truths_normalized_ft.append(true_answer_normalized)\n",
    "            original_indices_ft.append(current_batch_original_indices[pred_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:16:57.013601Z",
     "iopub.status.busy": "2025-05-16T03:16:57.013330Z",
     "iopub.status.idle": "2025-05-16T03:16:57.021618Z",
     "shell.execute_reply": "2025-05-16T03:16:57.020861Z",
     "shell.execute_reply.started": "2025-05-16T03:16:57.013585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results_ft_df = pd.DataFrame({\n",
    "    'original_index': original_indices_ft,\n",
    "    'predicted_answer_ft': predictions_ft,\n",
    "    'ground_truth_normalized': ground_truths_normalized_ft # Name kept same for consistency\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:16:57.022446Z",
     "iopub.status.busy": "2025-05-16T03:16:57.022269Z",
     "iopub.status.idle": "2025-05-16T03:16:57.054781Z",
     "shell.execute_reply": "2025-05-16T03:16:57.054131Z",
     "shell.execute_reply.started": "2025-05-16T03:16:57.022433Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_index</th>\n",
       "      <th>predicted_answer_ft</th>\n",
       "      <th>ground_truth_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7055</td>\n",
       "      <td>hard</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49141</td>\n",
       "      <td>gold</td>\n",
       "      <td>gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40947</td>\n",
       "      <td>three</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12004</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7905</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_index predicted_answer_ft ground_truth_normalized\n",
       "0            7055                hard                    hard\n",
       "1           49141                gold                    gold\n",
       "2           40947               three                   three\n",
       "3           12004                 yes                     yes\n",
       "4            7905                 two                     two"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ft_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:16:57.056460Z",
     "iopub.status.busy": "2025-05-16T03:16:57.056264Z",
     "iopub.status.idle": "2025-05-16T03:16:57.059842Z",
     "shell.execute_reply": "2025-05-16T03:16:57.059104Z",
     "shell.execute_reply.started": "2025-05-16T03:16:57.056445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eval_df = eval_df_for_final_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:16:57.061266Z",
     "iopub.status.busy": "2025-05-16T03:16:57.060768Z",
     "iopub.status.idle": "2025-05-16T03:16:57.146446Z",
     "shell.execute_reply": "2025-05-16T03:16:57.145886Z",
     "shell.execute_reply.started": "2025-05-16T03:16:57.061243Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned results saved to vqa_results_lora_finetuned.csv\n"
     ]
    }
   ],
   "source": [
    "df_with_ft_results = eval_df.merge(results_ft_df, left_index=True, right_on='original_index', how='right')\n",
    "\n",
    "results_ft_filename = 'vqa_results_lora_finetuned.csv'\n",
    "df_with_ft_results.to_csv(results_ft_filename, index=False)\n",
    "print(f\"Fine-tuned results saved to {results_ft_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:16:57.147348Z",
     "iopub.status.busy": "2025-05-16T03:16:57.147108Z",
     "iopub.status.idle": "2025-05-16T03:16:57.152768Z",
     "shell.execute_reply": "2025-05-16T03:16:57.152133Z",
     "shell.execute_reply.started": "2025-05-16T03:16:57.147328Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'question', 'answer', 'filename', 'original_index',\n",
       "       'predicted_answer_ft', 'ground_truth_normalized'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_ft_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T03:18:36.604366Z",
     "iopub.status.busy": "2025-05-16T03:18:36.604114Z",
     "iopub.status.idle": "2025-05-16T03:18:57.894425Z",
     "shell.execute_reply": "2025-05-16T03:18:57.893623Z",
     "shell.execute_reply.started": "2025-05-16T03:18:36.604351Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model Accuracy (Exact Match): 0.5701\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdfbdb473cb640d1bf62662d014ab0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1918409653a5436390139bdc62faa349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c694d0bb56e44dbb11ab2f5d4ce97c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74332c1b995841c994097d8d790aa5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23c35305cf342b88992c908629ea3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a36e6d7c13403aaa20d4130d86c905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc907e2b4f3403693e260bcd4e9815b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ BERTScore → P: 0.8724, R: 0.8459, F1: 0.8572\n",
      "▶ BERTScore1 → P1: 0.9785, R: 0.9740, F1: 0.9759\n",
      "LoRA Fine-tuning and Evaluation Script Finished.\n"
     ]
    }
   ],
   "source": [
    "# --- Calculate Metrics for Fine-tuned Model ---\n",
    "from bert_score import score\n",
    "import evaluate\n",
    "\n",
    "if not predictions_ft:\n",
    "    print(\"Error: No valid fine-tuned predictions available to calculate metrics.\")\n",
    "else:\n",
    "    correct_predictions_ft = sum(p == gt for p, gt in zip(predictions_ft, ground_truths_normalized_ft))\n",
    "    total_valid_ft = len(predictions_ft)\n",
    "    accuracy_ft = correct_predictions_ft / total_valid_ft if total_valid_ft > 0 else 0\n",
    "    pred= df_with_ft_results['predicted_answer_ft']\n",
    "    gt = df_with_ft_results['ground_truth_normalized']\n",
    "    print(f\"Fine-tuned Model Accuracy (Exact Match): {accuracy_ft:.4f}\")\n",
    "\n",
    "    # BERTScore (optional, if you want to compare)!\n",
    "    try:\n",
    "        bertscore = evaluate.load(\"bertscore\")\n",
    "        # Ensure predictions and references are lists of strings\n",
    "        # P, R, F1_bs  = score(pred.tolist(), gt.tolist(), lang='en', verbose=False, model_type='distilbert-base-uncased',rescale_with_baseline=True)\n",
    "        P, R, F1_bs  = score(pred.tolist(), gt.tolist(), lang='en', verbose=False,rescale_with_baseline=True)\n",
    "        P1, R1, F1_bs1  = score(pred.tolist(), gt.tolist(), lang='en', verbose=False)\n",
    "        # avg_f1_bertscore = sum(bertscore_results['f1']) / len(bertscore_results['f1']) if bertscore_results['f1'] else 0\n",
    "        print(f\"▶ BERTScore → P: {P.mean().item():.4f}, R: {R.mean().item():.4f}, F1: {F1_bs.mean().item():.4f}\")\n",
    "        print(f\"▶ BERTScore1 → P1: {P1.mean().item():.4f}, R: {R1.mean().item():.4f}, F1: {F1_bs1.mean().item():.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not compute BERTScore for fine-tuned model: {e}\")\n",
    "\n",
    "print(\"LoRA Fine-tuning and Evaluation Script Finished.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7430483,
     "sourceId": 11828111,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tdr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
