{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports and Environment Configuration\n",
    "In this section, we import essential libraries:\n",
    "- **pandas** for data manipulation using DataFrames.\n",
    "- **PIL (Python Imaging Library)** to handle image loading and conversion.\n",
    "- **torch** to leverage PyTorch for model inference.\n",
    "- **transformers** (via `processor` and `model`) for Visual Question Answering (VQA) functionality.\n",
    "We also configure device settings (CPU/GPU) for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T12:13:06.125605Z",
     "iopub.status.busy": "2025-05-13T12:13:06.124705Z",
     "iopub.status.idle": "2025-05-13T12:13:07.295647Z",
     "shell.execute_reply": "2025-05-13T12:13:07.295102Z",
     "shell.execute_reply.started": "2025-05-13T12:13:06.125563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import core libraries for data handling, image processing, and model inference\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import torch \n",
    "import evaluate \n",
    "from evaluate import load\n",
    "import warnings\n",
    "import pandas as pd \n",
    "from PIL import Image \n",
    "from tqdm.notebook import tqdm \n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Path Definition\n",
    "We define the constant `DATASET_CSV` to specify the CSV file location containing the VQA dataset. \n",
    "This CSV is expected to include columns for:\n",
    "- **image_path**: Path to each image file.\n",
    "- **question**: Natural language questions about the image.\n",
    "- **answer**: Ground-truth answers used for evaluation.\n",
    "Centralizing the path makes it easy to update file locations without modifying downstream code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T12:34:01.121342Z",
     "iopub.status.busy": "2025-05-13T12:34:01.121076Z",
     "iopub.status.idle": "2025-05-13T12:34:01.126178Z",
     "shell.execute_reply": "2025-05-13T12:34:01.125489Z",
     "shell.execute_reply.started": "2025-05-13T12:34:01.121321Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using Batch Size: 64\n"
     ]
    }
   ],
   "source": [
    "# Define the file path for the dataset CSV\n",
    "DATASET_CSV = '/kaggle/input/image-input/output.csv'\n",
    "IMAGE_BASE_DIR = '/kaggle/working/images/small'\n",
    "MODEL_NAME = \"Salesforce/blip-vqa-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Using Batch Size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T12:18:29.169476Z",
     "iopub.status.busy": "2025-05-13T12:18:29.168664Z",
     "iopub.status.idle": "2025-05-13T12:18:38.143440Z",
     "shell.execute_reply": "2025-05-13T12:18:38.142802Z",
     "shell.execute_reply.started": "2025-05-13T12:18:29.169438Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loaded 33866 samples.\n",
      "Loading model: Salesforce/blip-vqa-base...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc137a864664bceae5941cae43be069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45d8632991544b49b29a07a1155279d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab48da9463c4cb2af870c74deb2dc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4fc73cf541a43e08f7b11d2d46f7ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e393e562e3bc4e06aa557d5a301ac313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1712fd7e1c9345b6930ca1bb5d90a22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83006c9d94ce45d291ac6fd936bf2355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "try:\n",
    "    df = pd.read_csv(DATASET_CSV)\n",
    "    print(f\"Loaded {len(df)} samples.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {DATASET_CSV} not found\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}...\")\n",
    "processor = BlipProcessor.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = BlipForQuestionAnswering.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval() #Set model to evaluation mode \n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Question Answering Prediction Function\n",
    "This cell defines `get_vqa_prediction(image_path, question)`, which:\n",
    "1. **Loads and preprocesses** the image using PIL and converts it to RGB.\n",
    "2. **Processes inputs** by combining the image and question through the `processor`, returning PyTorch tensors.\n",
    "3. **Performs model inference** with `model.generate(...)` under `torch.no_grad()` to produce an answer (max 10 tokens).\n",
    "4. **Decodes** the generated token IDs back into a string with `processor.decode`.\n",
    "Error handling is included to gracefully manage missing or unreadable image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T12:19:46.460410Z",
     "iopub.status.busy": "2025-05-13T12:19:46.460189Z",
     "iopub.status.idle": "2025-05-13T12:19:46.466875Z",
     "shell.execute_reply": "2025-05-13T12:19:46.466030Z",
     "shell.execute_reply.started": "2025-05-13T12:19:46.460391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_vqa_prediction(image_path, question):\n",
    "    try:\n",
    "        raw_image = Image.open(image_path).convert('RGB')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Image not found at {image_path}\")\n",
    "        return \"[Image Not Found Error]\"\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error loading image {image_path}: {e}\")\n",
    "        return \"[Image Load Error]\"\n",
    "\n",
    "    inputs = processor(raw_image, question, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad(): # no gradients need to be calculated during inference\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10) # Limiting for single-word answers\n",
    "    answer = processor.decode(outputs[0], skip_special_tokens=True).strip() #decode answers\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T12:20:15.181114Z",
     "iopub.status.busy": "2025-05-13T12:20:15.180789Z",
     "iopub.status.idle": "2025-05-13T12:20:15.203791Z",
     "shell.execute_reply": "2025-05-13T12:20:15.203062Z",
     "shell.execute_reply.started": "2025-05-13T12:20:15.181093Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>718mYsQTQbL</td>\n",
       "      <td>What are the items in the image?</td>\n",
       "      <td>Bibs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718mYsQTQbL</td>\n",
       "      <td>What color is the solid bib?</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>718mYsQTQbL</td>\n",
       "      <td>How many bibs are shown?</td>\n",
       "      <td>Six</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>718mYsQTQbL</td>\n",
       "      <td>What material are the bibs?</td>\n",
       "      <td>Cotton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>718mYsQTQbL</td>\n",
       "      <td>Does one bib have a striped pattern?</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                              question  answer\n",
       "0  718mYsQTQbL      What are the items in the image?    Bibs\n",
       "1  718mYsQTQbL          What color is the solid bib?  Yellow\n",
       "2  718mYsQTQbL              How many bibs are shown?     Six\n",
       "3  718mYsQTQbL           What material are the bibs?  Cotton\n",
       "4  718mYsQTQbL  Does one bib have a striped pattern?     Yes"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T12:30:28.872165Z",
     "iopub.status.busy": "2025-05-13T12:30:28.871255Z",
     "iopub.status.idle": "2025-05-13T12:30:29.149751Z",
     "shell.execute_reply": "2025-05-13T12:30:29.148598Z",
     "shell.execute_reply.started": "2025-05-13T12:30:28.872133Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !gunzip /kaggle/working/images/metadata/images.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the `directory` variable pointing to the folder with image metadata files.\n",
    "Centralizing directory paths facilitates file management for batch processing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T12:31:02.796943Z",
     "iopub.status.busy": "2025-05-13T12:31:02.796256Z",
     "iopub.status.idle": "2025-05-13T12:31:03.250154Z",
     "shell.execute_reply": "2025-05-13T12:31:03.249508Z",
     "shell.execute_reply.started": "2025-05-13T12:31:02.796915Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "directory = \"/kaggle/working/listings/metadata\"\n",
    "df1 = pd.read_csv(r'/kaggle/working/images/metadata/images.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T12:33:37.096667Z",
     "iopub.status.busy": "2025-05-13T12:33:37.096210Z",
     "iopub.status.idle": "2025-05-13T12:33:37.130014Z",
     "shell.execute_reply": "2025-05-13T12:33:37.129111Z",
     "shell.execute_reply.started": "2025-05-13T12:33:37.096646Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4c/4c533ad7.jpg What are the items in the image? bibs\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    #Loops through each row in the DataFrame allowing row-wise operations such as per-image inference.\n",
    "    imageId = row['id']\n",
    "    question = row['question']\n",
    "    pt= df1[df1['image_id']==imageId]\n",
    "    pt= pt['path'].values[0]\n",
    "    true_answer = str(row['answer']).lower().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched Inference Loop\n",
    "\n",
    "- Calculate number of batches (`math.ceil(len(df)/BATCH_SIZE)`).\n",
    "- Disable gradients with `torch.no_grad()` for efficiency.\n",
    "- Loop over data in batches, loading and validating images and questions.\n",
    "- Skip empty batches or missing files, logging warnings.\n",
    "- Preprocess batch with `processor`, run `model.generate()`, and decode outputs.\n",
    "- Normalize predictions and ground truths (lowercase, strip, remove punctuation).\n",
    "- Store results and original indices for later evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T12:37:59.536816Z",
     "iopub.status.busy": "2025-05-13T12:37:59.536231Z",
     "iopub.status.idle": "2025-05-13T13:22:57.278792Z",
     "shell.execute_reply": "2025-05-13T13:22:57.277874Z",
     "shell.execute_reply.started": "2025-05-13T12:37:59.536792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batched inference...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c90b1dd077a4548b70873c692c6600b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Batches:   0%|          | 0/530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Running batched inference...\")\n",
    "predictions = []\n",
    "ground_truths_normalized = [] # Store normalized ground truths for metrics\n",
    "original_indices = []\n",
    "num_batches = math.ceil(len(df) / BATCH_SIZE)\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculations for inference\n",
    "    for i in tqdm(range(0, len(df), BATCH_SIZE), total=num_batches, desc=\"Evaluating Batches\"): #progress bar\n",
    "        batch_df = df[i:i+BATCH_SIZE] # Batch\n",
    "        batch_images_pil = []\n",
    "        batch_questions = []\n",
    "        batch_ground_truths = []\n",
    "        batch_valid_indices = [] \n",
    "\n",
    "        for idx, row in batch_df.iterrows(): # Load images and collect data for the current batch\n",
    "            imageId = row['id']\n",
    "            question = row['question']\n",
    "            pt = df1[df1['image_id']==imageId]\n",
    "            pt = pt['path'].values[0]\n",
    "            true_answer = str(row['answer']).lower().strip()\n",
    "            img_path = os.path.join(IMAGE_BASE_DIR, pt) #img_path, true_answer and question for each batch loaded \n",
    "\n",
    "            try:\n",
    "                raw_image = Image.open(img_path).convert('RGB') #reading the image, appeding \n",
    "                batch_images_pil.append(raw_image)\n",
    "                batch_questions.append(question)\n",
    "                batch_ground_truths.append(true_answer)\n",
    "                batch_valid_indices.append(idx) \n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: Image not found at {img_path}. Skipping row {idx}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error loading image {img_path} for row {idx}: {e}. Skipping.\")\n",
    "\n",
    "        # 2. Process the batch if any valid images were loaded\n",
    "        if not batch_images_pil:\n",
    "            print(f\"Warning: No valid images loaded for batch starting at index {i}. Skipping batch.\")\n",
    "            continue # Skip to the next batch\n",
    "\n",
    "        # Use the processor for the entire batch\n",
    "        inputs = processor(images=batch_images_pil, text=batch_questions, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "\n",
    "        # 3. Generate answers for the batch\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "        # 4. Decode and store results for the batch\n",
    "        batch_preds_decoded = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        for pred_idx, original_df_idx in enumerate(batch_valid_indices):\n",
    "            # Normalize prediction\n",
    "            predicted_answer = batch_preds_decoded[pred_idx].strip().lower()\n",
    "            predicted_answer = re.sub(r'[^\\w\\s]', '', predicted_answer) # Basic cleanup\n",
    "\n",
    "            # Normalize corresponding ground truth\n",
    "            true_answer_normalized = batch_ground_truths[pred_idx] # Already lowercased/stripped\n",
    "            true_answer_normalized = re.sub(r'[^\\w\\s]', '', true_answer_normalized) # Basic cleanup\n",
    "\n",
    "            predictions.append(predicted_answer)\n",
    "            ground_truths_normalized.append(true_answer_normalized)\n",
    "            original_indices.append(original_df_idx) # Store the original index\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results DataFrame Construction\n",
    "Aggregates prediction results into a new `results_df` DataFrame by passing a dictionary of lists.\n",
    "This structure standardizes output for downstream analysis or export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:36:18.374403Z",
     "iopub.status.busy": "2025-05-13T13:36:18.373832Z",
     "iopub.status.idle": "2025-05-13T13:36:18.535861Z",
     "shell.execute_reply": "2025-05-13T13:36:18.535002Z",
     "shell.execute_reply.started": "2025-05-13T13:36:18.374375Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to vqa_results_baseline_batched.csv\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'original_index': original_indices,\n",
    "    'predicted_answer': predictions,\n",
    "    'ground_truth_normalized': ground_truths_normalized\n",
    "})\n",
    "# Ensure the original df has a unique index if it was reset during sampling\n",
    "df_with_results = df.merge(results_df, left_index=True, right_on='original_index', how='right') # right join to keep only processed rows\n",
    "results_filename = 'vqa_results_baseline_batched.csv'\n",
    "df_with_results.to_csv(results_filename, index=False)\n",
    "print(f\"Results saved to {results_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"../VR-mini-Proj-2/BLIP_vqa_results_baseline_batched.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:37:25.791255Z",
     "iopub.status.busy": "2025-05-13T13:37:25.790541Z",
     "iopub.status.idle": "2025-05-13T13:37:25.802207Z",
     "shell.execute_reply": "2025-05-13T13:37:25.801650Z",
     "shell.execute_reply.started": "2025-05-13T13:37:25.791230Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\varni\\miniconda3\\envs\\tdr\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Exact Match): 0.4248\n"
     ]
    }
   ],
   "source": [
    "valid_predictions = results_df['predicted_answer'].to_list()\n",
    "valid_ground_truths = results_df['ground_truth_normalized'].to_list()\n",
    "\n",
    "if not valid_predictions:\n",
    "    print(\"Error: No valid predictions available to calculate metrics.\")\n",
    "    exit()\n",
    "\n",
    "# 1. Accuracy (Exact Match)\n",
    "correct_predictions = sum(p == gt for p, gt in zip(valid_predictions, valid_ground_truths))\n",
    "total_valid = len(valid_predictions)\n",
    "accuracy = correct_predictions / total_valid if total_valid > 0 else 0\n",
    "print(f\"Accuracy (Exact Match): {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:38:02.396149Z",
     "iopub.status.busy": "2025-05-13T13:38:02.395437Z",
     "iopub.status.idle": "2025-05-13T13:38:02.399811Z",
     "shell.execute_reply": "2025-05-13T13:38:02.399258Z",
     "shell.execute_reply.started": "2025-05-13T13:38:02.396128Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Macro, based on Exact Match): 0.4248\n"
     ]
    }
   ],
   "source": [
    "f1_macro_simple = accuracy\n",
    "print(f\"F1 Score (Macro, based on Exact Match): {f1_macro_simple:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing BERT Score with model `distilbert-base-uncased`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load(\"bertscore\")\n",
    "results = bertscore.compute(references=valid_ground_truths, predictions=valid_predictions,lang=\"en\",model_type=\"distilbert-base-uncased\",rescale_with_baseline=True) \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6075401236502055\n",
      "0.6054488398469636\n",
      "0.6065456961339067\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.mean(results['precision']))\n",
    "print(np.mean(results['recall']))\n",
    "print(np.mean(results['f1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Embedding and Cosine Similarity\n",
    "\n",
    "- **Objective**: Quantify similarity between model predictions and ground-truth answers using TF-IDF representations and cosine similarity.\n",
    "- **Steps**:  \n",
    "  1. Initialize a `TfidfVectorizer` and fit it on the combined text of valid predictions and ground truths.  \n",
    "  2. Transform each set into dense vectors and convert to PyTorch tensors (`pred_vec`, `gt_vec`).  \n",
    "  3. Compute pairwise cosine similarity (`F.cosine_similarity`) across corresponding prediction–truth vectors.  \n",
    "- **Output**: A tensor of cosine similarity scores, indicating how closely each predicted answer matches its reference.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:56:45.771519Z",
     "iopub.status.busy": "2025-05-13T13:56:45.770702Z",
     "iopub.status.idle": "2025-05-13T13:56:48.745503Z",
     "shell.execute_reply": "2025-05-13T13:56:48.744830Z",
     "shell.execute_reply.started": "2025-05-13T13:56:45.771485Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: tensor([1., 1., 0.,  ..., 1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "all_sentences = valid_predictions + valid_ground_truths\n",
    "vectorizer.fit(all_sentences)\n",
    "\n",
    "vec1 = vectorizer.transform(valid_predictions).toarray()\n",
    "vec2 = vectorizer.transform(valid_ground_truths).toarray()\n",
    "\n",
    "pred_vec = torch.tensor(vec1, dtype=torch.float32)\n",
    "gt_vec = torch.tensor(vec2, dtype=torch.float32)\n",
    "\n",
    "\n",
    "cos_sim = F.cosine_similarity(pred_vec, gt_vec, dim=1)\n",
    "print(\"Cosine similarity:\", cos_sim)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7406801,
     "sourceId": 11795298,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tdr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
